"""LLM å®¢æˆ·ç«¯ä½¿ç”¨ç¤ºä¾‹

æ­¤æ–‡ä»¶æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ LLM å®¢æˆ·ç«¯ä¸ä¸åŒçš„æ¨¡å‹æä¾›å•†è¿›è¡Œäº¤äº’ã€‚
"""

import asyncio
import os
from dotenv import load_dotenv

from agentscope.message import Msg
from configs.config import ModelConfig
from services.llm import LLMClient


async def basic_chat_example():
    """åŸºæœ¬èŠå¤©ç¤ºä¾‹"""
    print("=" * 50)
    print("åŸºæœ¬èŠå¤©ç¤ºä¾‹")
    print("=" * 50)
    
    # ä»ç¯å¢ƒå˜é‡åˆ›å»ºé…ç½®
    config = ModelConfig.from_env(provider="dashscope")
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = LLMClient(config)
    
    # åˆ›å»ºæ¶ˆæ¯
    messages = [
        Msg(name="user", content="ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹è‡ªå·±", role="user")
    ]
    
    # å‘é€èŠå¤©è¯·æ±‚
    print("\nå‘é€æ¶ˆæ¯: ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹è‡ªå·±")
    response = await client.chat(messages, reuse_history=False)
    
    # å¤„ç†å“åº”
    print("\næ¨¡å‹å›å¤:")
    for block in response.content:
        if block.get("type") == "text":
            print(block.get("text"))
    
    print("\n" + "=" * 50 + "\n")


async def chat_with_history_example():
    """å¸¦å†å²è®°å½•çš„å¤šè½®å¯¹è¯ç¤ºä¾‹"""
    print("=" * 50)
    print("å¤šè½®å¯¹è¯ç¤ºä¾‹ï¼ˆå¸¦å†å²ï¼‰")
    print("=" * 50)
    
    config = ModelConfig.from_env(provider="dashscope")
    client = LLMClient(config)
    
    # ç¬¬ä¸€è½®å¯¹è¯
    messages1 = [Msg(name="user", content="æˆ‘å«å¼ ä¸‰", role="user")]
    print("\nç¬¬ä¸€è½® - ç”¨æˆ·: æˆ‘å«å¼ ä¸‰")
    response1 = await client.chat(messages1, reuse_history=True)
    
    print("åŠ©æ‰‹:", end=" ")
    for block in response1.content:
        if block.get("type") == "text":
            print(block.get("text"))
    
    # ç¬¬äºŒè½®å¯¹è¯ï¼ˆä¼šåŒ…å«ç¬¬ä¸€è½®çš„å†å²ï¼‰
    messages2 = [Msg(name="user", content="æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ", role="user")]
    print("\nç¬¬äºŒè½® - ç”¨æˆ·: æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")
    response2 = await client.chat(messages2, reuse_history=True)
    
    print("åŠ©æ‰‹:", end=" ")
    for block in response2.content:
        if block.get("type") == "text":
            print(block.get("text"))
    
    print("\n" + "=" * 50 + "\n")


async def custom_parameters_example():
    """è‡ªå®šä¹‰æ¨¡å‹å‚æ•°ç¤ºä¾‹"""
    print("=" * 50)
    print("è‡ªå®šä¹‰å‚æ•°ç¤ºä¾‹ï¼ˆé«˜æ¸©åº¦ = æ›´æœ‰åˆ›æ„ï¼‰")
    print("=" * 50)
    
    # åˆ›å»ºé«˜æ¸©åº¦é…ç½®ï¼ˆæ›´æœ‰åˆ›æ„ï¼‰
    config = ModelConfig.from_env(provider="dashscope")
    config.temperature = 0.95
    config.top_p = 0.9
    config.generate_kwargs = {
        "temperature": 0.95,
        "top_p": 0.9,
    }
    
    client = LLMClient(config)
    
    messages = [
        Msg(name="user", content="ç”¨ä¸€ä¸ªè¯å½¢å®¹æ˜¥å¤©", role="user")
    ]
    
    print(f"\næ¨¡å‹å‚æ•°: temperature={config.temperature}, top_p={config.top_p}")
    print("\nå‘é€æ¶ˆæ¯: ç”¨ä¸€ä¸ªè¯å½¢å®¹æ˜¥å¤©")
    response = await client.chat(messages, reuse_history=False)
    
    print("\næ¨¡å‹å›å¤:")
    for block in response.content:
        if block.get("type") == "text":
            print(block.get("text"))
    
    print("\n" + "=" * 50 + "\n")


async def streaming_chat_example():
    """æµå¼å“åº”ç¤ºä¾‹"""
    print("=" * 50)
    print("æµå¼å“åº”ç¤ºä¾‹ï¼ˆå®æ—¶æ˜¾ç¤ºï¼‰")
    print("=" * 50)
    
    config = ModelConfig.from_env(provider="dashscope")
    # ç¡®ä¿å¯ç”¨æµå¼
    config.stream = True
    
    client = LLMClient(config)
    
    messages = [
        Msg(name="user", content="è¯·æ•°æ•°å­—ä»1åˆ°10", role="user")
    ]
    
    print("\nå‘é€æ¶ˆæ¯: è¯·æ•°æ•°å­—ä»1åˆ°10")
    print("\næ¨¡å‹å›å¤ï¼ˆæµå¼ï¼‰:", end=" ", flush=True)
    
    # ä½¿ç”¨ chat_stream æ–¹æ³•è·å–æµå¼å“åº”
    if hasattr(client.client, "chat_stream"):
        async for chunk in client.client.chat_stream(messages, config, reuse_history=False):
            # æå–å¹¶æ˜¾ç¤ºæ–‡æœ¬
            for block in chunk.content:
                if block.get("type") == "text":
                    # åªæ‰“å°æ–°å¢çš„éƒ¨åˆ†ï¼ˆå®é™…ä½¿ç”¨ä¸­å¯èƒ½éœ€è¦åšå·®åˆ†ï¼‰
                    print(block.get("text", ""), end="", flush=True)
        print()  # æ¢è¡Œ
    else:
        print("\nå½“å‰å®¢æˆ·ç«¯ä¸æ”¯æŒæµå¼å“åº”æ–¹æ³•")
    
    print("\n" + "=" * 50 + "\n")


async def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("é”™è¯¯: æœªæ‰¾åˆ° DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
        print("è¯·å¤åˆ¶ .env.example åˆ° .env å¹¶å¡«å†™æ‚¨çš„ API å¯†é’¥")
        return
    
    print("\n")
    print("ğŸš€ LLM å®¢æˆ·ç«¯ä½¿ç”¨ç¤ºä¾‹")
    print("\n")
    
    try:
        # è¿è¡Œå„ç§ç¤ºä¾‹
        await basic_chat_example()
        await chat_with_history_example()
        await custom_parameters_example()
        await streaming_chat_example()
        
        print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())

