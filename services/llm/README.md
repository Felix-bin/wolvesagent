# LLM æœåŠ¡æ¨¡å—

æ­¤æ¨¡å—æä¾›äº†ä¸å„ç§ LLM æä¾›å•†ï¼ˆå¦‚ DashScopeã€OpenAIã€Anthropicï¼‰äº¤äº’çš„ç»Ÿä¸€æ¥å£ï¼ŒåŸºäº [AgentScope](https://github.com/modelscope/agentscope) æ¡†æ¶å®ç°ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸ”Œ **å¤šæä¾›å•†æ”¯æŒ**ï¼šç»Ÿä¸€æ¥å£æ”¯æŒ DashScopeã€OpenAIã€Anthropic ç­‰å¤šä¸ª LLM æä¾›å•†
- âš™ï¸ **çµæ´»é…ç½®**ï¼šä»ç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶è¯»å–æ¨¡å‹å‚æ•°ï¼ˆtemperatureã€max_tokens ç­‰ï¼‰
- ğŸ› ï¸ **å·¥å…·è°ƒç”¨**ï¼šæ”¯æŒå‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰åŠŸèƒ½
- ğŸ’­ **æ€ç»´é“¾**ï¼šæ”¯æŒå¯ç”¨æ¨¡å‹æ€ç»´æ¨¡å¼ï¼ˆå¦‚ DeepSeek-R1ã€QwQ ç­‰ï¼‰
- ğŸ“ **å†å²ç®¡ç†**ï¼šå†…ç½®èŠå¤©å†å²ç®¡ç†åŠŸèƒ½
- ğŸ”„ **æµå¼è¾“å‡º**ï¼šæ”¯æŒæµå¼å“åº”è¾“å‡º

## å®‰è£…

ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–ï¼š

```bash
# ä½¿ç”¨ uvï¼ˆæ¨èï¼‰
uv sync

# æˆ–ä½¿ç”¨ pip
pip install agentscope
```

## å¿«é€Ÿå¼€å§‹

### 1. é…ç½®ç¯å¢ƒå˜é‡

å¤åˆ¶ `.env.example` åˆ° `.env` å¹¶å¡«å†™æ‚¨çš„ API å¯†é’¥ï¼š

```bash
cp .env.example .env
```

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼š

```env
# é€‰æ‹©æä¾›å•†: dashscope, openai, anthropic
LLM_PROVIDER=dashscope

# DashScope é…ç½®
DASHSCOPE_API_KEY=your_api_key_here
DASHSCOPE_MODEL_NAME=qwen3-max

# æ¨¡å‹å‚æ•°
MODEL_TEMPERATURE=0.7
MODEL_STREAM=true
```

### 2. åŸºæœ¬ä½¿ç”¨

```python
import asyncio
from agentscope.message import Msg
from configs.config import ModelConfig
from services.llm import LLMClient

async def main():
    # ä»ç¯å¢ƒå˜é‡åˆ›å»ºé…ç½®
    config = ModelConfig.from_env(provider="dashscope")
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = LLMClient(config)
    
    # åˆ›å»ºæ¶ˆæ¯
    messages = [
        Msg(name="user", content="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±", role="user")
    ]
    
    # å‘é€èŠå¤©è¯·æ±‚
    response = await client.chat(messages)
    
    # å¤„ç†å“åº”
    for block in response.content:
        if block.get("type") == "text":
            print(block.get("text"))

asyncio.run(main())
```

### 3. ä½¿ç”¨å·¥å…·è°ƒç”¨

```python
import asyncio
from agentscope.message import Msg
from agentscope.tool import Toolkit, ToolResponse
from configs.config import ModelConfig
from services.llm import LLMClient

# å®šä¹‰å·¥å…·å‡½æ•°ï¼ˆå¿…é¡»è¿”å› ToolResponseï¼‰
async def get_weather(city: str) -> ToolResponse:
    """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯
    
    Args:
        city: åŸå¸‚åç§°
        
    Returns:
        åŒ…å«å¤©æ°”ä¿¡æ¯çš„ ToolResponse
    """
    weather_info = f"{city} ä»Šå¤©æ™´å¤©ï¼Œæ¸©åº¦ 20-25Â°C"
    return ToolResponse(
        content=[{"type": "text", "text": weather_info}]
    )

async def main():
    # åˆå§‹åŒ–é…ç½®å’Œå®¢æˆ·ç«¯
    config = ModelConfig.from_env(provider="dashscope")
    client = LLMClient(config)
    
    # åˆ›å»ºå·¥å…·åŒ…å¹¶æ³¨å†Œå‡½æ•°
    toolkit = Toolkit()
    toolkit.register_tool_function(get_weather)
    
    # è·å–å·¥å…·çš„ JSON schemasï¼ˆè¿”å› list[dict]ï¼‰
    tools = toolkit.get_json_schemas()
    
    # åˆ›å»ºæ¶ˆæ¯
    messages = [
        Msg(name="user", content="åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ", role="user")
    ]
    
    # å¸¦å·¥å…·çš„èŠå¤©è¯·æ±‚
    response = await client.chat(messages, tools=tools)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
    for block in response.content:
        if block.get("type") == "tool_use":
            # æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆtoolkit.call_tool_function è¿”å› AsyncGeneratorï¼‰
            async for tool_result in toolkit.call_tool_function(block):
                # å¤„ç†å·¥å…·ç»“æœï¼ˆç´¯ç§¯çš„ï¼‰
                if tool_result.is_last:
                    print(f"å·¥å…·è°ƒç”¨ç»“æœ: {tool_result.content}")

asyncio.run(main())
```

### 4. ç®¡ç†èŠå¤©å†å²

```python
import asyncio
from agentscope.message import Msg
from configs.config import ModelConfig
from services.llm import LLMClient

async def main():
    config = ModelConfig.from_env(provider="dashscope")
    client = LLMClient(config)
    
    # ç¬¬ä¸€è½®å¯¹è¯
    messages1 = [Msg(name="user", content="æˆ‘å«å¼ ä¸‰", role="user")]
    response1 = await client.chat(messages1, reuse_history=True)
    
    # ç¬¬äºŒè½®å¯¹è¯ï¼ˆä¼šåŒ…å«ç¬¬ä¸€è½®çš„å†å²ï¼‰
    messages2 = [Msg(name="user", content="æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ", role="user")]
    response2 = await client.chat(messages2, reuse_history=True)
    
    # æ¨¡å‹åº”è¯¥èƒ½è®°ä½ä¹‹å‰è¯´çš„åå­—

asyncio.run(main())
```

### 5. è‡ªå®šä¹‰æ¨¡å‹å‚æ•°

```python
import asyncio
from agentscope.message import Msg
from configs.config import ModelConfig, ModelProviderConfig
from services.llm import LLMClient

async def main():
    # æ–¹å¼ 1: ä»ç¯å¢ƒå˜é‡åˆ›å»ºå¹¶ä¿®æ”¹å‚æ•°
    config = ModelConfig.from_env(provider="dashscope")
    config.temperature = 0.9  # è°ƒæ•´æ¸©åº¦
    config.max_tokens = 2000  # è®¾ç½®æœ€å¤§ token æ•°
    
    # æ–¹å¼ 2: æ‰‹åŠ¨åˆ›å»ºé…ç½®
    provider_config = ModelProviderConfig(
        provider="dashscope",
        api_key="your_api_key",
        base_url=None
    )
    
    config = ModelConfig(
        model_name="qwen3-max",
        model_provider=provider_config,
        temperature=0.8,
        max_tokens=4000,
        top_p=0.95,
        stream=True,
        generate_kwargs={
            "temperature": 0.8,
            "max_tokens": 4000,
            "top_p": 0.95
        }
    )
    
    client = LLMClient(config)
    messages = [Msg(name="user", content="ç”Ÿæˆä¸€ä¸ªåˆ›æ„æ•…äº‹", role="user")]
    response = await client.chat(messages)

asyncio.run(main())
```

## æ¶æ„è®¾è®¡

```
services/llm/
â”œâ”€â”€ __init__.py          # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ base.py              # BaseLLMClient åŸºç±»
â”œâ”€â”€ client.py            # LLMClient ä¸»å®¢æˆ·ç«¯å’Œ LLMProvider æšä¸¾
â”œâ”€â”€ dashscope.py         # DashScope å®¢æˆ·ç«¯å®ç°
â””â”€â”€ README.md            # æœ¬æ–‡æ¡£
```

### ç±»å…³ç³»

```
BaseLLMClient (æŠ½è±¡åŸºç±»)
    â†‘
    â”‚ ç»§æ‰¿
    â”‚
DashScopeClient (å…·ä½“å®ç°)
    â†‘
    â”‚ ä½¿ç”¨
    â”‚
LLMClient (é—¨é¢æ¨¡å¼)
```

## æ·»åŠ æ–°çš„æä¾›å•†

è¦æ·»åŠ æ–°çš„ LLM æä¾›å•†ï¼ˆå¦‚ OpenAIã€Anthropicï¼‰ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ï¼š

1. **åˆ›å»ºå®¢æˆ·ç«¯å®ç°**ï¼šåœ¨ `services/llm/` ä¸‹åˆ›å»ºæ–°æ–‡ä»¶ï¼ˆå¦‚ `openai.py`ï¼‰

```python
from agentscope.model import OpenAIChatModel, ChatResponse
from agentscope.message import Msg
from services.llm.base import BaseLLMClient
from configs.config import ModelConfig

class OpenAIClient(BaseLLMClient):
    """ä½¿ç”¨ agentscope å®ç°çš„ OpenAI å®¢æˆ·ç«¯"""
    
    def __init__(self, model_config: ModelConfig):
        super().__init__(model_config)
        self.model = OpenAIChatModel(
            api_key=self.api_key,
            model_name=model_config.model_name,
            stream=model_config.stream,
            generate_kwargs=model_config.generate_kwargs,
        )
        self._chat_history = []
    
    def set_chat_history(self, messages):
        # å®ç°å†å²ç®¡ç†
        pass
    
    async def chat(self, messages, model_config, tools=None, reuse_history=True):
        # å®ç°èŠå¤©é€»è¾‘
        pass
```

2. **æ›´æ–° client.py**ï¼šåœ¨ `LLMClient.__init__` ä¸­æ·»åŠ æ–°çš„ case

```python
case LLMProvider.OpenAI:
    from .openai import OpenAIClient
    self.client: BaseLLMClient = OpenAIClient(model_config)
```

3. **æ›´æ–°é…ç½®**ï¼šåœ¨ `ModelConfig.from_env` ä¸­æ·»åŠ æ–°æä¾›å•†çš„ç¯å¢ƒå˜é‡å¤„ç†

## API å‚è€ƒ

### ModelConfig

æ¨¡å‹é…ç½®ç±»ï¼Œç”¨äºç®¡ç† LLM æ¨¡å‹çš„æ‰€æœ‰é…ç½®å‚æ•°ã€‚

**å­—æ®µï¼š**
- `model_name`: æ¨¡å‹åç§°ï¼ˆå¦‚ "qwen3-max"ï¼‰
- `model_provider`: æä¾›å•†é…ç½®
- `temperature`: æ¸©åº¦å‚æ•°ï¼ˆ0.0-1.0ï¼‰
- `max_tokens`: æœ€å¤§ç”Ÿæˆ token æ•°
- `top_p`: Top-p é‡‡æ ·å‚æ•°
- `stream`: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
- `enable_thinking`: æ˜¯å¦å¯ç”¨æ€ç»´æ¨¡å¼
- `supports_tool_calling`: æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨
- `generate_kwargs`: é¢å¤–çš„ç”Ÿæˆå‚æ•°å­—å…¸

**æ–¹æ³•ï¼š**
- `from_env(provider: str) -> ModelConfig`: ä»ç¯å¢ƒå˜é‡åˆ›å»ºé…ç½®

### LLMClient

ä¸» LLM å®¢æˆ·ç«¯ç±»ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£è®¿é—®ä¸åŒçš„ LLM æä¾›å•†ã€‚

**æ–¹æ³•ï¼š**

#### `__init__(model_config: ModelConfig)`
åˆå§‹åŒ–å®¢æˆ·ç«¯

#### `async chat(messages, model_config=None, tools=None, reuse_history=True) -> ChatResponse`
å‘é€èŠå¤©æ¶ˆæ¯

**å‚æ•°ï¼š**
- `messages`: Msg å¯¹è±¡åˆ—è¡¨
- `model_config`: å¯é€‰çš„é…ç½®è¦†ç›–
- `tools`: å¯é€‰çš„å·¥å…· JSON schemas åˆ—è¡¨ï¼ˆä» `Toolkit.get_json_schemas()` è·å–ï¼‰
- `reuse_history`: æ˜¯å¦é‡ç”¨å†å²

**è¿”å›ï¼š**
- `ChatResponse`: æ¨¡å‹å“åº”

#### `set_chat_history(messages: list[ChatResponse])`
è®¾ç½®èŠå¤©å†å²

#### `supports_tool_calling(model_config=None) -> bool`
æ£€æŸ¥æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨

## ç¯å¢ƒå˜é‡

| å˜é‡å | è¯´æ˜ | é»˜è®¤å€¼ |
|--------|------|--------|
| `DASHSCOPE_API_KEY` | DashScope API å¯†é’¥ | å¿…å¡« |
| `DASHSCOPE_MODEL_NAME` | DashScope æ¨¡å‹åç§° | qwen3-max |
| `DASHSCOPE_BASE_URL` | DashScope API åŸºç¡€ URL | å¯é€‰ |
| `MODEL_TEMPERATURE` | æ¸©åº¦å‚æ•° | 0.7 |
| `MODEL_MAX_TOKENS` | æœ€å¤§ token æ•° | å¯é€‰ |
| `MODEL_TOP_P` | Top-p é‡‡æ · | å¯é€‰ |
| `MODEL_STREAM` | å¯ç”¨æµå¼è¾“å‡º | true |
| `MODEL_ENABLE_THINKING` | å¯ç”¨æ€ç»´æ¨¡å¼ | å¯é€‰ |

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•å¤„ç†æµå¼å“åº”ï¼Ÿ

A: LLM å®¢æˆ·ç«¯æä¾›ä¸¤ç§æ–¹å¼å¤„ç†æµå¼å“åº”ï¼š

**æ–¹å¼ 1: ä½¿ç”¨ `chat()` æ–¹æ³•ï¼ˆè‡ªåŠ¨å¤„ç†æµå¼ï¼‰**

`chat()` æ–¹æ³•ä¼šè‡ªåŠ¨å¤„ç†æµå¼å“åº”ï¼Œè¿”å›æœ€ç»ˆçš„å®Œæ•´ `ChatResponse`ï¼š

```python
config = ModelConfig.from_env(provider="dashscope")
config.stream = True  # å¯ç”¨æµå¼

client = LLMClient(config)
response = await client.chat(messages)  # è‡ªåŠ¨æ”¶é›†æ‰€æœ‰æµå¼å—

# ç›´æ¥ä½¿ç”¨å®Œæ•´å“åº”
for block in response.content:
    if block.get("type") == "text":
        print(block.get("text"))
```

**æ–¹å¼ 2: ä½¿ç”¨ `chat_stream()` æ–¹æ³•ï¼ˆå®æ—¶æµå¼ï¼‰**

å¦‚æœéœ€è¦å®æ—¶å¤„ç†æµå¼å“åº”ï¼ˆä¾‹å¦‚å®æ—¶æ˜¾ç¤ºè¾“å‡ºï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ `chat_stream()` æ–¹æ³•ï¼š

```python
config = ModelConfig.from_env(provider="dashscope")
config.stream = True

client = LLMClient(config)

# å®æ—¶å¤„ç†æµå¼å“åº”
async for chunk in client.client.chat_stream(messages, config):
    for block in chunk.content:
        if block.get("type") == "text":
            print(block.get("text"), end="", flush=True)
print()  # æ¢è¡Œ
```

æ³¨æ„ï¼š`chat_stream()` è¿”å›çš„æ˜¯ç´¯ç§¯çš„å“åº”å—ï¼Œæ¯ä¸ªå—éƒ½åŒ…å«ä»å¼€å§‹åˆ°å½“å‰çš„å®Œæ•´å†…å®¹ã€‚

### Q: å¦‚ä½•å¯ç”¨æ€ç»´æ¨¡å¼ï¼Ÿ

A: åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® `MODEL_ENABLE_THINKING=true`ï¼Œæˆ–åœ¨ä»£ç ä¸­ï¼š

```python
config = ModelConfig.from_env(provider="dashscope")
config.enable_thinking = True
```

### Q: å¦‚ä½•å¤„ç†å·¥å…·è°ƒç”¨çš„å“åº”ï¼Ÿ

A: å·¥å…·å‡½æ•°å¿…é¡»è¿”å› `ToolResponse` å¯¹è±¡ï¼Œä½¿ç”¨ `Toolkit.call_tool_function()` æ¥æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼š

```python
from agentscope.tool import Toolkit, ToolResponse

# å·¥å…·å‡½æ•°å¿…é¡»è¿”å› ToolResponse
async def my_tool(arg: str) -> ToolResponse:
    result = f"å¤„ç†äº†: {arg}"
    return ToolResponse(content=[{"type": "text", "text": result}])

# æ³¨å†Œå·¥å…·
toolkit = Toolkit()
toolkit.register_tool_function(my_tool)

# è·å– JSON schemas ä¼ é€’ç»™æ¨¡å‹
tools = toolkit.get_json_schemas()
response = await client.chat(messages, tools=tools)

# å¤„ç†å·¥å…·è°ƒç”¨
for block in response.content:
    if block.get("type") == "tool_use":
        # call_tool_function è¿”å› AsyncGenerator
        async for tool_result in toolkit.call_tool_function(block):
            if tool_result.is_last:
                print(tool_result.content)
```

## å‚è€ƒèµ„æ–™

- [AgentScope æ–‡æ¡£](https://agentscope.io/)
- [DashScope API æ–‡æ¡£](https://help.aliyun.com/zh/dashscope/)
- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs)
- [Anthropic API æ–‡æ¡£](https://docs.anthropic.com/)

## License

æœ¬é¡¹ç›®éµå¾ªé¡¹ç›®æ ¹ç›®å½•çš„ LICENSE æ–‡ä»¶ã€‚

