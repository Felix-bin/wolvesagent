# LLM æœåŠ¡æ¨¡å—

æ­¤æ¨¡å—æä¾›äº†ä¸å„ç§ LLM æä¾›å•†ï¼ˆå¦‚ DashScopeã€OpenAIã€Anthropicï¼‰äº¤äº’çš„ç»Ÿä¸€æ¥å£ï¼ŒåŸºäº [AgentScope](https://github.com/modelscope/agentscope) æ¡†æ¶å®ç°ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸ”Œ **å¤šæä¾›å•†æ”¯æŒ**ï¼šç»Ÿä¸€æ¥å£æ”¯æŒ DashScopeã€OpenAIã€Anthropic ç­‰å¤šä¸ª LLM æä¾›å•†
- âš™ï¸ **çµæ´»é…ç½®**ï¼šä»ç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶è¯»å–æ¨¡å‹å‚æ•°ï¼ˆtemperatureã€max_tokens ç­‰ï¼‰
- ğŸ› ï¸ **å·¥å…·è°ƒç”¨**ï¼šæ”¯æŒå‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰åŠŸèƒ½
- ğŸ’­ **æ€ç»´é“¾**ï¼šæ”¯æŒå¯ç”¨æ¨¡å‹æ€ç»´æ¨¡å¼ï¼ˆå¦‚ DeepSeek-R1ã€QwQ ç­‰ï¼‰
- ğŸ“ **å†å²ç®¡ç†**ï¼šå†…ç½®èŠå¤©å†å²ç®¡ç†åŠŸèƒ½
- ğŸ”„ **æµå¼è¾“å‡º**ï¼šæ”¯æŒæµå¼å“åº”è¾“å‡º

## å¿«é€Ÿå¼€å§‹

### é…ç½®ç¯å¢ƒå˜é‡

åˆ›å»º`.env`æ–‡ä»¶æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ï¼š

```env
# DashScope é…ç½®
DASHSCOPE_API_KEY=your_api_key_here
DASHSCOPE_MODEL_NAME=qwen3-max

# æ¨¡å‹å‚æ•°
MODEL_TEMPERATURE=0.7
MODEL_STREAM=false  # å…¼å®¹ç°æœ‰é¡¹ç›®
```

### åŸºæœ¬ä½¿ç”¨

```python
import asyncio
from agentscope.message import Msg
from config.models import create_llm_client

async def main():
    # ä»ç¯å¢ƒå˜é‡åˆ›å»ºå®¢æˆ·ç«¯
    client = create_llm_client()
    
    # åˆ›å»ºæ¶ˆæ¯
    messages = [
        Msg(name="user", content="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±", role="user")
    ]
    
    # å‘é€èŠå¤©è¯·æ±‚
    response = await client.chat(messages)
    
    # å¤„ç†å“åº”
    for block in response.content:
        if block.get("type") == "text":
            print(block.get("text"))

asyncio.run(main())
```

## æ¶æ„è®¾è®¡

```
services/llm/
â”œâ”€â”€ __init__.py          # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ base.py              # BaseLLMClient åŸºç±»
â”œâ”€â”€ client.py            # LLMClient ä¸»å®¢æˆ·ç«¯å’Œ LLMProvider æšä¸¾
â”œâ”€â”€ dashscope.py         # DashScope å®¢æˆ·ç«¯å®ç°
â””â”€â”€ README.md            # æœ¬æ–‡æ¡£
```

### ç±»å…³ç³»

```
BaseLLMClient (æŠ½è±¡åŸºç±»)
    â†‘
    â”‚ ç»§æ‰¿
    â”‚
DashScopeClient (å…·ä½“å®ç°)
    â†‘
    â”‚ ä½¿ç”¨
    â”‚
LLMClient (é—¨é¢æ¨¡å¼)
```

## API å‚è€ƒ

### create_llm_client()

åˆ›å»ºLLMå®¢æˆ·ç«¯çš„ä¾¿æ·å‡½æ•°ï¼ˆæ¨èä½¿ç”¨ï¼‰

**å‚æ•°ï¼š**
- `provider`: æä¾›å•†åç§°ï¼ˆé»˜è®¤"dashscope"ï¼‰
- `model_name`: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
- `from_env`: æ˜¯å¦ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼ˆé»˜è®¤Trueï¼‰

**è¿”å›ï¼š**
- `LLMClient`: é…ç½®å¥½çš„å®¢æˆ·ç«¯å®ä¾‹

**ç¤ºä¾‹ï¼š**
```python
# ä½¿ç”¨é»˜è®¤é…ç½®
client = create_llm_client()

# æŒ‡å®šæ¨¡å‹
client = create_llm_client(model_name="deepseek-v3")
```

### LLMClient.chat()

å‘é€èŠå¤©æ¶ˆæ¯

**å‚æ•°ï¼š**
- `messages`: Msg å¯¹è±¡åˆ—è¡¨
- `model_config`: å¯é€‰çš„é…ç½®è¦†ç›–
- `tools`: å¯é€‰çš„å·¥å…· JSON schemas åˆ—è¡¨
- `reuse_history`: æ˜¯å¦é‡ç”¨å†å²ï¼ˆé»˜è®¤Trueï¼‰

**è¿”å›ï¼š**
- `ChatResponse`: æ¨¡å‹å“åº”

## ç¯å¢ƒå˜é‡

| å˜é‡å | è¯´æ˜ | é»˜è®¤å€¼ |
|--------|------|--------|
| `DASHSCOPE_API_KEY` | DashScope API å¯†é’¥ | å¿…å¡« |
| `DASHSCOPE_MODEL_NAME` | DashScope æ¨¡å‹åç§° | qwen-max |
| `MODEL_TEMPERATURE` | æ¸©åº¦å‚æ•° | 0.7 |
| `MODEL_MAX_TOKENS` | æœ€å¤§ token æ•° | å¯é€‰ |
| `MODEL_STREAM` | å¯ç”¨æµå¼è¾“å‡º | false |

## æ›´å¤šä¿¡æ¯

è¯¦è§ï¼š
- [LLMæœåŠ¡é›†æˆæ–‡æ¡£](../../docs/LLM_SERVICE_INTEGRATION.md)
- [AgentScope æ–‡æ¡£](https://agentscope.io/)
- [DashScope API æ–‡æ¡£](https://help.aliyun.com/zh/dashscope/)




